{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Exists2025:  Sexism Identification in Twitter</h1>\n",
    "<h2 align=\"center\">Task 1. Fine-tuning for binary classification\n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">ETSInf. Universitat Politècnica de València</h3>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import  AutoTokenizer, AutoModelForSequenceClassification,  Trainer, TrainingArguments,  EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "#Importing the required modules to use the ICM measure\n",
    "from pyevall.evaluation import PyEvALLEvaluation\n",
    "from pyevall.metrics.metricfactory import MetricFactory\n",
    "from pyevall.reports.reports import PyEvALLReport\n",
    "from pyevall.utils.utils import PyEvALLUtils\n",
    "from functools import partial\n",
    "\n",
    "from readerEXIST2025 import EXISTReader\n",
    "from create_submision_folder import create_submision_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r\"\\bhttps?://[\\w.-]+(?:\\.[a-zA-Z]{2,})?(?:/\\S*/?)?\"\n",
    "\n",
    "emojis = sorted(emoji.EMOJI_DATA, key=len, reverse=True)\n",
    "pattern_emoji = \"(\" + \"|\".join(re.escape(u) for u in emojis) + \")\"\n",
    "\n",
    "pattern_all = r\"|\".join(\n",
    "    [\n",
    "        url,\n",
    "        pattern_emoji,\n",
    "    ]\n",
    ")\n",
    "\n",
    "re_all = re.compile(pattern_all, re.U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = \"data/EXIST 2025 Tweets Dataset/training/EXIST2025_training.json\"\n",
    "file_dev = \"data/EXIST 2025 Tweets Dataset/dev/EXIST2025_dev.json\"\n",
    "file_test = \"data/EXIST 2025 Tweets Dataset/test/EXIST2025_test_clean.json\"\n",
    "\n",
    "reader_train = EXISTReader(file_train)\n",
    "reader_dev = EXISTReader(file_dev)\n",
    "reader_test = EXISTReader(file_test, is_test=True)\n",
    "\n",
    "EnTrainTask3, EnDevTask3, EnTestTask3 = reader_train.get(lang=\"EN\", subtask=\"3\", regular_exp=re_all, preprocess=True), reader_dev.get(lang=\"EN\", subtask=\"3\", regular_exp=re_all, preprocess=True), reader_test.get(lang=\"EN\", subtask=\"3\", regular_exp=re_all, preprocess=True)\n",
    "SpTrainTask3, SpDevTask3, SpTestTask3 = reader_train.get(lang=\"ES\", subtask=\"3\", regular_exp=re_all, preprocess=True), reader_dev.get(lang=\"ES\", subtask=\"3\", regular_exp=re_all, preprocess=True), reader_test.get(lang=\"ES\", subtask=\"3\", regular_exp=re_all, preprocess=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICMWrapper(pred, labels, multi=False,ids=None):\n",
    "    test = PyEvALLEvaluation()\n",
    "    metrics=[MetricFactory.ICM.value]\n",
    "    params= dict()\n",
    "    fillLabel=None\n",
    "    if multi:\n",
    "        params[PyEvALLUtils.PARAM_REPORT]=\"embedded\"\n",
    "        hierarchy={\"True\":['IDEOLOGICAL-INEQUALITY', 'STEREOTYPING-DOMINANCE', 'MISOGYNY-NON-SEXUAL-VIOLENCE', 'OBJECTIFICATION', 'SEXUAL-VIOLENCE'],\n",
    "        \"False\":[]}\n",
    "        params[PyEvALLUtils.PARAM_HIERARCHY]=hierarchy\n",
    "        fillLabel = lambda x: [\"False\"] if len(x)== 0 else x\n",
    "    else:\n",
    "        params[PyEvALLUtils.PARAM_REPORT]=\"simple\"\n",
    "        fillLabel = lambda x: str(x)\n",
    "\n",
    "\n",
    "    truth_name, predict_name=None, None\n",
    "    if ids is None:\n",
    "        ids=list(range(len(labels)))\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(mode='w', delete=False, encoding='utf-8') as truth:\n",
    "        truth_name=truth.name\n",
    "        truth_df=pd.DataFrame({'test_case': ['EXIST2025']*len(labels),\n",
    "                        'id': [str(x) for x in ids],\n",
    "                        'value': [fillLabel(x) for x in labels]})\n",
    "        if multi==True:\n",
    "            truth_df=truth_df.astype('object')\n",
    "        truth.write(truth_df.to_json(orient=\"records\"))\n",
    "\n",
    "    with  tempfile.NamedTemporaryFile(mode='w', delete=False) as predict:\n",
    "        predict_name=predict.name\n",
    "        predict_df=pd.DataFrame({'test_case': ['EXIST2025']*len(pred),\n",
    "                        'id': [str(x) for x in ids],\n",
    "                        'value': [fillLabel(x) for x in pred]})\n",
    "        if multi==True:\n",
    "            predict_df=predict_df.astype('object')\n",
    "        predict.write(predict_df.to_json(orient=\"records\"))\n",
    "\n",
    "    report = test.evaluate(predict_name, truth_name, metrics, **params)\n",
    "    os.unlink(truth_name)\n",
    "    os.unlink(predict_name)\n",
    "\n",
    "    icm = None\n",
    "    if 'metrics' in report.report:\n",
    "        if 'ICM' in report.report[\"metrics\"]: icm=float(report.report[\"metrics\"]['ICM'][\"results\"][\"average_per_test_case\"])\n",
    "    return icm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=1234):\n",
    "    \"\"\"\n",
    "    Sets the seed to make everything deterministic, for reproducibility of experiments\n",
    "    Parameters:\n",
    "    seed: the number to set the seed to\n",
    "    Return: None\n",
    "    \"\"\"\n",
    "    # Random seed\n",
    "    random.seed(seed)\n",
    "    # Numpy seed\n",
    "    np.random.seed(seed)\n",
    "    # Torch seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # os seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SexismDatasetMulti(Dataset):\n",
    "    def __init__(self, texts, labels, ids, tokenizer, max_len=128, pad=\"max_length\", trunc=True,rt='pt'):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels\n",
    "        self.ids = ids\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.pad = pad\n",
    "        self.trunc = trunc\n",
    "        self.rt = rt\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,padding=self.pad, truncation=self.trunc,\n",
    "            return_tensors=self.rt\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float),\n",
    "            'id': torch.tensor(self.ids[idx], dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_3(pred, lencoder):\n",
    "    labels = pred.label_ids\n",
    "    #preds = pred.predictions.argmax(-1)\n",
    "    preds = torch.sigmoid(torch.tensor(pred.predictions)).numpy()\n",
    "    preds_binary = (preds >= 0.5).astype(int)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds_binary, average=None, zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds_binary)\n",
    "    icm= ICMWrapper(lencoder.inverse_transform(preds_binary), lencoder.inverse_transform(labels), multi=True)\n",
    "    # Macro averages\n",
    "    precision_macro = np.mean(precision)\n",
    "    recall_macro = np.mean(recall)\n",
    "    f1_macro = np.mean(f1)\n",
    "    metrics = {}\n",
    "    metrics.update({\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'ICM': icm\n",
    "    })\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sexism_classification_pipeline_task3(trainInfo, devInfo, testInfo=None, model_name='roberta-base', nlabels=5, ptype=\"multi_label_classification\", **args):\n",
    "    # Model and Tokenizer\n",
    "    labelEnc= MultiLabelBinarizer()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=nlabels,\n",
    "        problem_type=ptype, \n",
    "        ignore_mismatched_sizes=args.get(\"ignore_mismatched_sizes\", False)\n",
    "        )\n",
    "\n",
    "    # Prepare datasets\n",
    "    train_dataset = SexismDatasetMulti(trainInfo[1], labelEnc.fit_transform(trainInfo[2]),[int(x) for x in trainInfo[0]], tokenizer )\n",
    "    val_dataset = SexismDatasetMulti(devInfo[1], labelEnc.transform(devInfo[2]), [int(x) for x in devInfo[0]], tokenizer)\n",
    "\n",
    "    # Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        report_to=\"none\", # alt: \"wandb\", \"tensorboard\" \"comet_ml\" \"mlflow\" \"clearml\"\n",
    "        output_dir= args.get('output_dir', './results'),\n",
    "        num_train_epochs= args.get('num_train_epochs', 5),\n",
    "        learning_rate=args.get('learning_rate', 5e-5),\n",
    "        per_device_train_batch_size=args.get('per_device_train_batch_size', 16),\n",
    "        per_device_eval_batch_size=args.get('per_device_eval_batch_size', 64),\n",
    "        warmup_steps=args.get('warmup_steps', 500),\n",
    "        weight_decay=args.get('weight_decay',0.01),\n",
    "        logging_dir=args.get('logging_dir', './logs'),\n",
    "        logging_steps=args.get('logging_steps', 10),\n",
    "        eval_strategy=args.get('eval_strategy','epoch'),\n",
    "        save_strategy=args.get('save_strategy', \"epoch\"),\n",
    "        save_total_limit=args.get('save_total_limit', 1),\n",
    "        load_best_model_at_end=args.get('load_best_model_at_end', True),\n",
    "        metric_for_best_model=args.get('metric_for_best_model',\"ICM\")\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        #compute_metrics=compute_metrics_3,\n",
    "        compute_metrics = partial(compute_metrics_3, lencoder=labelEnc),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=args.get(\"early_stopping_patience\",3))]\n",
    "    )\n",
    "\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(\"Validation Results:\", eval_results)\n",
    "\n",
    "    if testInfo is not None:\n",
    "      # Prepare test dataset for prediction\n",
    "      test_dataset = SexismDatasetMulti(testInfo[1], [[0,0,0,0,0]] * len(testInfo[1]),  [int(x) for x in testInfo[0]],   tokenizer)\n",
    "\n",
    "      # Predict test set labels\n",
    "      predictions = trainer.predict(test_dataset)\n",
    "      #predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "      predicted_probs = torch.sigmoid(torch.tensor(predictions.predictions)).numpy()\n",
    "      predicted_labels = (predicted_probs >= 0.5).astype(int)\n",
    "\n",
    "      # Create submission DataFrame\n",
    "      submission_df = pd.DataFrame({\n",
    "          'id': testInfo[0],\n",
    "          'label': labelEnc.inverse_transform(predicted_labels),\n",
    "          \"test_case\": [\"EXIST2025\"]*len(predicted_labels)\n",
    "\n",
    "      })\n",
    "      submission_df.to_csv('sexism_predictions_task3.csv', index=False)\n",
    "      print(\"Prediction TASK3 completed. Results saved to sexism_predictions_task2.csv\")\n",
    "      return model, submission_df\n",
    "    return model, eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='231' max='660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [231/660 04:01 < 07:31, 0.95 it/s, Epoch 7/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Icm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.665200</td>\n",
       "      <td>0.649672</td>\n",
       "      <td>0.561034</td>\n",
       "      <td>0.498970</td>\n",
       "      <td>0.469083</td>\n",
       "      <td>-0.900066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.614800</td>\n",
       "      <td>0.586054</td>\n",
       "      <td>0.710605</td>\n",
       "      <td>0.707351</td>\n",
       "      <td>0.699644</td>\n",
       "      <td>0.104957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.549300</td>\n",
       "      <td>0.594340</td>\n",
       "      <td>0.730410</td>\n",
       "      <td>0.617405</td>\n",
       "      <td>0.664798</td>\n",
       "      <td>-0.783211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.493000</td>\n",
       "      <td>0.593407</td>\n",
       "      <td>0.691654</td>\n",
       "      <td>0.758509</td>\n",
       "      <td>0.722505</td>\n",
       "      <td>0.017081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.426100</td>\n",
       "      <td>0.614890</td>\n",
       "      <td>0.711442</td>\n",
       "      <td>0.713774</td>\n",
       "      <td>0.710129</td>\n",
       "      <td>-0.193747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>0.660142</td>\n",
       "      <td>0.727232</td>\n",
       "      <td>0.708142</td>\n",
       "      <td>0.711253</td>\n",
       "      <td>-0.049251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.755934</td>\n",
       "      <td>0.717010</td>\n",
       "      <td>0.688820</td>\n",
       "      <td>0.699214</td>\n",
       "      <td>0.015883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-23 18:07:59,040 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:07:59,085 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "cargado 29\n",
      "2025-04-23 18:08:33,109 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:08:33,148 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2025-04-23 18:09:07,437 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:09:07,476 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2025-04-23 18:09:41,953 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:09:41,996 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2025-04-23 18:10:17,147 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:10:17,186 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2025-04-23 18:10:52,289 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:10:52,330 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2025-04-23 18:11:26,565 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:11:26,607 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-23 18:11:30,681 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:11:30,722 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "Validation Results: {'eval_loss': 0.5860541462898254, 'eval_precision_macro': 0.7106050093237422, 'eval_recall_macro': 0.7073505602567571, 'eval_f1_macro': 0.6996441522945052, 'eval_ICM': 0.10495684106643186, 'eval_runtime': 1.8014, 'eval_samples_per_second': 187.636, 'eval_steps_per_second': 3.331, 'epoch': 7.0}\n",
      "2025-04-23 18:11:35,456 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:11:35,547 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "Prediction TASK3 completed. Results saved to sexism_predictions_task2.csv\n"
     ]
    }
   ],
   "source": [
    "modelname = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "params = {\"num_train_epochs\": 20,\n",
    "          \"learning_rate\": 0.0001,\n",
    "          \"per_device_train_batch_size\": 64,\n",
    "          \"warmup_steps\": 200,\n",
    "          \"early_stopping_patience\": 5,\n",
    "          \"ignore_mismatched_sizes\": True,\n",
    "          \"logging_dir\": None,\n",
    "          \"language\": \"english\"\n",
    "          }\n",
    "\n",
    "_, eval_results = sexism_classification_pipeline_task3(EnTrainTask3, EnDevTask3, EnTestTask3, modelname, 5, \"multi_label_classification\", **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/800 07:06 < 07:08, 0.93 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Icm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.648800</td>\n",
       "      <td>0.658545</td>\n",
       "      <td>0.484574</td>\n",
       "      <td>0.711421</td>\n",
       "      <td>0.572352</td>\n",
       "      <td>-0.212001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.582600</td>\n",
       "      <td>0.592964</td>\n",
       "      <td>0.700108</td>\n",
       "      <td>0.698671</td>\n",
       "      <td>0.669933</td>\n",
       "      <td>0.075833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.534800</td>\n",
       "      <td>0.557457</td>\n",
       "      <td>0.719699</td>\n",
       "      <td>0.740940</td>\n",
       "      <td>0.719817</td>\n",
       "      <td>0.243792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.436500</td>\n",
       "      <td>0.570780</td>\n",
       "      <td>0.731828</td>\n",
       "      <td>0.723512</td>\n",
       "      <td>0.723467</td>\n",
       "      <td>0.211905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.347800</td>\n",
       "      <td>0.620014</td>\n",
       "      <td>0.714015</td>\n",
       "      <td>0.744476</td>\n",
       "      <td>0.722195</td>\n",
       "      <td>0.278668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.234500</td>\n",
       "      <td>0.702498</td>\n",
       "      <td>0.740536</td>\n",
       "      <td>0.698693</td>\n",
       "      <td>0.713919</td>\n",
       "      <td>0.158825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.142200</td>\n",
       "      <td>0.800688</td>\n",
       "      <td>0.719236</td>\n",
       "      <td>0.707004</td>\n",
       "      <td>0.710646</td>\n",
       "      <td>0.092316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>0.953550</td>\n",
       "      <td>0.727712</td>\n",
       "      <td>0.684123</td>\n",
       "      <td>0.691749</td>\n",
       "      <td>0.068952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.042600</td>\n",
       "      <td>1.044353</td>\n",
       "      <td>0.696136</td>\n",
       "      <td>0.713857</td>\n",
       "      <td>0.703277</td>\n",
       "      <td>0.130324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>1.107942</td>\n",
       "      <td>0.702621</td>\n",
       "      <td>0.707898</td>\n",
       "      <td>0.701564</td>\n",
       "      <td>0.091612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-23 18:12:27,322 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:12:27,367 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2025-04-23 18:13:07,737 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:13:07,783 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2025-04-23 18:13:47,810 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:13:47,855 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2025-04-23 18:14:27,759 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:14:27,803 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2025-04-23 18:15:08,681 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:15:08,727 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2025-04-23 18:15:48,600 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:15:48,645 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2025-04-23 18:16:29,633 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:16:29,679 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2025-04-23 18:17:20,420 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:17:20,464 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2025-04-23 18:18:04,820 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:18:04,863 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "2025-04-23 18:18:53,755 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:18:53,803 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-23 18:18:57,801 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:18:57,847 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "Validation Results: {'eval_loss': 0.6200137734413147, 'eval_precision_macro': 0.714015422673501, 'eval_recall_macro': 0.7444756519749317, 'eval_f1_macro': 0.7221948546623337, 'eval_ICM': 0.27866838511131264, 'eval_runtime': 2.0249, 'eval_samples_per_second': 194.085, 'eval_steps_per_second': 3.457, 'epoch': 10.0}\n",
      "2025-04-23 18:19:03,188 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
      "2025-04-23 18:19:03,289 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
      "Prediction TASK3 completed. Results saved to sexism_predictions_task2.csv\n"
     ]
    }
   ],
   "source": [
    "modelname = \"pysentimiento/robertuito-sentiment-analysis\"\n",
    "params = {\"num_train_epochs\": 20,\n",
    "          \"learning_rate\": 0.0001,\n",
    "          \"per_device_train_batch_size\": 64,\n",
    "          \"warmup_steps\": 200,\n",
    "          \"early_stopping_patience\": 5, \n",
    "          \"logging_dir\": None,\n",
    "          \"ignore_mismatched_sizes\": True,\n",
    "          \"language\": \"spanish\"\n",
    "          }\n",
    "\n",
    "_, eval_results = sexism_classification_pipeline_task3(SpTrainTask3, SpDevTask3, SpTestTask3, modelname, 5, \"multi_label_classification\", **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved\n"
     ]
    }
   ],
   "source": [
    "if create_submision_file(\"ArPa\", 1, \"hard\", 1, 3, \".\", \"exist2025_ArPa\"):\n",
    "    print(\"Predictions saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
